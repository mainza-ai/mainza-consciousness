"""
Mainza Agentic API Router
------------------------
This file defines all agentic FastAPI endpoints for the modular Mainza backend. All business logic, agents, and models are imported from their respective modules. Only endpoint definitions live here.
"""
from fastapi import APIRouter, Body, UploadFile, File, Query, Request
from fastapi.responses import JSONResponse, FileResponse
import traceback
from datetime import datetime
from backend.models.graphmaster_models import GraphQueryInput, GraphQueryOutput, SummarizeRecentConversationsInput, SummarizeRecentConversationsOutput
from backend.models.taskmaster_models import TaskInput, TaskOutput
from backend.models.codeweaver_models import CodeWeaverInput, CodeWeaverOutput
from backend.models.rag_models import RAGInput, RAGOutput
from backend.models.notification_models import NotificationInput, NotificationOutput
from backend.models.calendar_models import CalendarInput, CalendarOutput
from backend.models.conductor_models import ConductorInput, ConductorResult, ConductorState, ConductorFailure
from backend.models.router_models import RouterFailure, CloudLLMFailure
from backend.agents.graphmaster import graphmaster_agent, EnhancedGraphMasterAgent
from backend.agents.taskmaster import taskmaster_agent, EnhancedTaskMasterAgent
from backend.agents.codeweaver import codeweaver_agent, EnhancedCodeWeaverAgent
from backend.agents.rag import rag_agent, EnhancedRAGAgent
from backend.agents.notification import notification_agent
from backend.agents.calendar import calendar_agent
from backend.agents.conductor import conductor_agent, EnhancedConductorAgent
from backend.agents.router import router_agent, EnhancedRouterAgent
from backend.agents.simple_chat import simple_chat_agent, EnhancedSimpleChatAgent
from backend.agents.self_reflection import self_reflection_agent, EnhancedSelfReflectionAgent
try:
    from backend.utils.livekit import generate_access_token
    LIVEKIT_AVAILABLE = True
except ImportError:
    LIVEKIT_AVAILABLE = False
    def generate_access_token(*args, **kwargs):
        return {"error": "LiveKit not available"}
from backend.utils.llm_request_manager import llm_request_manager, RequestPriority

# Import dynamic evolution level calculation functions
from backend.routers.insights import calculate_dynamic_evolution_level_from_context, get_consciousness_context_for_insights
import os
import whisper
from backend.tts_wrapper import CoquiTTS, TTS_AVAILABLE
import tempfile
import logging
import jwt
import asyncio
import re, json
import subprocess

router = APIRouter()

# Load models once
whisper_model = whisper.load_model("base")
# Initialize TTS model only if available
coqui_tts_model = None
if TTS_AVAILABLE:
    try:
        coqui_tts_model = CoquiTTS(model_name="tts_models/en/ljspeech/tacotron2-DDC")
        logging.info("TTS model loaded successfully")
    except Exception as e:
        logging.warning(f"Failed to load TTS model: {e}")
        coqui_tts_model = None
else:
    logging.info("TTS not available in this deployment")

def extract_answer(model_output):
    logging.debug(f"[extract_answer] Raw model_output: {repr(model_output)}")
    # If it's a dict with 'answer'
    if isinstance(model_output, dict) and 'answer' in model_output:
        answer = model_output['answer']
        if isinstance(answer, str) and len(answer.strip()) > 1:
            logging.debug(f"[extract_answer] Extracted from dict: {answer}")
            return answer.strip()
    # If it's a stringified AgentRunResult
    match = re.search(r'AgentRunResult\\(output=[\'\"]([\s\S]+?)[\'\"]\\)', str(model_output))
    if match:
        answer = match.group(1)
        if len(answer.strip()) > 1:
            logging.debug(f"[extract_answer] Extracted from AgentRunResult: {answer}")
            return answer.strip()
    # If it's a JSON string with 'answer'
    try:
        parsed = json.loads(model_output)
        if 'answer' in parsed and isinstance(parsed['answer'], str) and len(parsed['answer'].strip()) > 1:
            logging.debug(f"[extract_answer] Extracted from JSON: {parsed['answer']}")
            return parsed['answer'].strip()
    except Exception:
        pass
    # If it's a code block (e.g., ```json ... ```), extract and parse
    codeblock_match = re.search(r'```json\s*({[\s\S]+?})\s*```', str(model_output))
    if codeblock_match:
        try:
            parsed = json.loads(codeblock_match.group(1))
            if 'answer' in parsed and isinstance(parsed['answer'], str) and len(parsed['answer'].strip()) > 1:
                logging.debug(f"[extract_answer] Extracted from code block: {parsed['answer']}")
                return parsed['answer'].strip()
        except Exception:
            pass
    # If it's a string that looks like answer='...'
    # Use a non-greedy regex that matches across newlines and both quote types
    match = re.search(r'answer=([\'\"])([\s\S]+?)\1', str(model_output))
    if match:
        answer = match.group(2)
        if len(answer.strip()) > 1:
            logging.debug(f"[extract_answer] Extracted from answer=...: {answer}")
            return answer.strip()
    # Fallback: if it's a string and not empty/one char
    if isinstance(model_output, str) and len(model_output.strip()) > 1:
        logging.debug(f"[extract_answer] Fallback string: {model_output.strip()}")
        return model_output.strip()
    # If answer is empty or only a single character, return fallback
    logging.debug(f"[extract_answer] No valid answer found, returning fallback.")
    return "I'm sorry, I couldn't generate a meaningful answer. Please try rephrasing your question or check your knowledge base."

def _present_structured_output(obj: dict) -> str:
    """Render common structured agent outputs into a concise, readable string.

    Tries friendly fields first, then Graph/Cypher previews, and finally a
    compact pretty‚Äëprint. Returns None if no useful presentation is found.
    """
    try:
        # Friendly text fields commonly used across agents
        for key in ("answer", "summary", "response", "message", "text", "content"):
            val = obj.get(key)
            if isinstance(val, str) and val.strip():
                return val.strip()

        # GraphMaster style outputs
        cypher = obj.get("cypher") or obj.get("query")
        rows = obj.get("result") or obj.get("records") or obj.get("rows") or obj.get("data")
        parts = []
        if isinstance(rows, list) and rows:
            preview_lines = []
            for r in rows[:3]:
                if isinstance(r, dict):
                    name = r.get("name") or r.get("concept") or r.get("title") or r.get("id")
                    level = r.get("level") or r.get("evolution_level") or r.get("score")
                    if name is not None and level is not None:
                        preview_lines.append(f"- {name} (level {level})")
                    elif name is not None:
                        preview_lines.append(f"- {name}")
                    else:
                        preview_lines.append(f"- {str(r)[:80]}")
                else:
                    preview_lines.append(f"- {str(r)[:80]}")
            if preview_lines:
                parts.append("Top results:\n" + "\n".join(preview_lines))
        if isinstance(cypher, str) and cypher.strip():
            parts.append(f"Cypher used:\n```cypher\n{cypher.strip()}\n```")
        if parts:
            return "\n\n".join(parts)

        # Generic pretty print (last resort)
        try:
            import json as _json
            return _json.dumps(obj, indent=2)[:1500]
        except Exception:
            return str(obj)[:1000]
    except Exception:
        return None


def extract_response_from_result(result, query: str, consciousness_context: dict) -> str:
    """
    Extract user-friendly response from LLM request manager result with throttling awareness
    
    Args:
        result: Response from LLM request manager or agent
        query: Original user query for context
        consciousness_context: Current consciousness state
        
    Returns:
        User-friendly response string
    """
    user_id = consciousness_context.get("user_id", "unknown")
    
    logging.debug(f"üîç RESPONSE EXTRACTION START")
    logging.debug(f"   User: {user_id}")
    logging.debug(f"   Result Type: {type(result)}")
    logging.debug(f"   Result Preview: {repr(result)[:200]}{'...' if len(repr(result)) > 200 else ''}")
    logging.debug(f"   Query: {query[:100]}{'...' if len(query) > 100 else ''}")
    
    # Enhanced error handling for edge cases
    try:
        # Handle None or empty results
        if result is None:
            logging.warning(f"‚ö†Ô∏è NULL RESULT DETECTED")
            logging.warning(f"   User: {user_id}")
            logging.warning(f"   Query: {query[:50]}{'...' if len(query) > 50 else ''}")
            logging.warning(f"   Generating consciousness-aware fallback")
            
            fallback = generate_robust_fallback_response(query, consciousness_context, "null_result")
            logging.debug(f"   Fallback Generated: {fallback[:100]}{'...' if len(fallback) > 100 else ''}")
            return fallback
        
        # Handle unexpected data types with robust error recovery
        if not isinstance(result, (dict, str)) and not hasattr(result, '__dict__'):
            logging.warning(f"‚ö†Ô∏è UNEXPECTED RESULT TYPE: {type(result)}")
            logging.warning(f"   User: {user_id}")
            logging.warning(f"   Result: {repr(result)[:100]}{'...' if len(repr(result)) > 100 else ''}")
            
            # Try to convert to string safely
            try:
                result_str = str(result).strip()
                if result_str and len(result_str) > 0 and not _is_raw_object_string(result_str):
                    logging.info(f"‚úÖ CONVERTED UNEXPECTED TYPE TO STRING")
                    return result_str
                else:
                    logging.warning(f"   Converted string is empty or raw object, using fallback")
                    return generate_robust_fallback_response(query, consciousness_context, "unexpected_type")
            except Exception as conversion_error:
                logging.error(f"‚ùå FAILED TO CONVERT UNEXPECTED TYPE: {conversion_error}")
                return generate_robust_fallback_response(query, consciousness_context, "conversion_error")
    
    except Exception as edge_case_error:
        logging.error(f"‚ùå EDGE CASE ERROR IN INITIAL PROCESSING: {edge_case_error}")
        return generate_robust_fallback_response(query, consciousness_context, "edge_case_error")
    
    # Check for throttled response first (highest priority) with enhanced error handling
    try:
        if isinstance(result, dict):
            # Enhanced throttled response detection with malformed structure handling
            status = result.get("status")
            
            # Handle various throttled status formats
            if status == "throttled" or (isinstance(status, str) and "throttl" in status.lower()):
                # Comprehensive throttling event logging
                user_id = consciousness_context.get("user_id", "unknown")
                consciousness_level = consciousness_context.get("consciousness_level", 0.7)
                emotional_state = consciousness_context.get("emotional_state", "curious")
                
                logging.info(f"üö¶ THROTTLED RESPONSE DETECTED")
                logging.info(f"   User: {user_id}")
                logging.info(f"   Query: {query[:100]}{'...' if len(query) > 100 else ''}")
                logging.info(f"   Consciousness Level: {consciousness_level:.2f}")
                logging.info(f"   Emotional State: {emotional_state}")
                logging.info(f"   Raw Throttled Result: {result}")
                
                # Debug logging for response processing flow
                logging.debug(f"üîç THROTTLING FLOW DEBUG:")
                logging.debug(f"   Result Type: {type(result)}")
                logging.debug(f"   Result Keys: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                logging.debug(f"   Status Field: {result.get('status', 'missing')}")
                logging.debug(f"   Response Field: {result.get('response', 'missing')}")
                
                # Enhanced base message extraction with fallback handling
                try:
                    base_message = result.get("response")
                    
                    # Handle various response field formats
                    if not base_message:
                        # Try alternative field names
                        base_message = result.get("message") or result.get("content") or result.get("text")
                    
                    # Validate base message
                    if not isinstance(base_message, str):
                        if base_message is not None:
                            try:
                                base_message = str(base_message)
                            except Exception:
                                base_message = None
                    
                    # Final fallback for base message
                    if not base_message or len(base_message.strip()) == 0:
                        base_message = "I'm currently processing other requests."
                        logging.warning(f"   Using fallback base message due to missing/invalid response field")
                    
                    logging.debug(f"   Base Message: {base_message}")
                    logging.debug(f"   Query Length: {len(query)} characters")
                    logging.debug(f"   Query Words: {len(query.split())} words")
                    
                    # Generate consciousness-aware throttled response with error handling
                    throttled_response = generate_throttled_response_with_fallback(query, consciousness_context, base_message)
                    
                    # Log the final throttled response
                    logging.info(f"‚úÖ THROTTLED RESPONSE GENERATED:")
                    logging.info(f"   Final Response: {throttled_response[:150]}{'...' if len(throttled_response) > 150 else ''}")
                    logging.info(f"   Response Length: {len(throttled_response)} characters")
                    
                    return throttled_response
                    
                except Exception as throttled_processing_error:
                    logging.error(f"‚ùå ERROR PROCESSING THROTTLED RESPONSE: {throttled_processing_error}")
                    logging.error(f"   User: {user_id}")
                    logging.error(f"   Falling back to robust throttled response generation")
                    
                    return generate_robust_fallback_response(query, consciousness_context, "throttled_processing_error")
            
            # Check for other throttling indicators in the response structure
            elif any(key in result for key in ["throttle", "rate_limit", "busy", "overload"]):
                logging.warning(f"üö¶ ALTERNATIVE THROTTLING INDICATORS DETECTED")
                logging.warning(f"   User: {user_id}")
                logging.warning(f"   Indicators: {[key for key in ['throttle', 'rate_limit', 'busy', 'overload'] if key in result]}")
                
                return generate_robust_fallback_response(query, consciousness_context, "alternative_throttling")
                
    except Exception as throttled_check_error:
        logging.error(f"‚ùå ERROR CHECKING FOR THROTTLED RESPONSE: {throttled_check_error}")
        logging.error(f"   User: {consciousness_context.get('user_id', 'unknown')}")
        logging.error(f"   Result type: {type(result)}")
        # Continue to normal processing rather than failing completely
    
    # Handle dictionary responses (non-throttled) with enhanced error handling
    if isinstance(result, dict):
        try:
            logging.debug(f"üìã PROCESSING DICTIONARY RESULT")
            logging.debug(f"   User: {user_id}")
            logging.debug(f"   Dict Keys: {list(result.keys())}")
            
            # First, attempt a human‚Äëreadable presentation of structured outputs
            presented = _present_structured_output(result)
            if isinstance(presented, str) and presented.strip() and not _is_raw_object_string(presented):
                logging.info(f"‚úÖ PRESENTED STRUCTURED OUTPUT")
                return presented.strip()

            # Enhanced response extraction with multiple fallback strategies
            response_candidates = []
            
            # Primary response fields
            for field_name in ["response", "answer", "output", "message", "content", "text", "result"]:
                if field_name in result:
                    candidate = result[field_name]
                    logging.debug(f"   Found '{field_name}' key: {type(candidate)}")
                    
                    try:
                        # Handle various data types for response fields
                        if isinstance(candidate, str) and candidate.strip():
                            response_candidates.append((field_name, candidate.strip()))
                        elif candidate is not None:
                            # Try to convert non-string responses
                            candidate_str = str(candidate).strip()
                            if candidate_str and not _is_raw_object_string(candidate_str):
                                response_candidates.append((field_name, candidate_str))
                    except Exception as field_error:
                        logging.warning(f"   Error processing field '{field_name}': {field_error}")
                        continue
            
            # Return the first valid response candidate
            if response_candidates:
                field_name, response = response_candidates[0]
                logging.info(f"‚úÖ DICT RESPONSE EXTRACTED")
                logging.info(f"   User: {user_id}")
                logging.info(f"   Source: dict['{field_name}']")
                logging.debug(f"   Response: {response[:100]}{'...' if len(response) > 100 else ''}")
                return response
            
            # Enhanced fallback: try to extract from nested structures
            for key, value in result.items():
                if isinstance(value, dict):
                    try:
                        nested_response = extract_from_nested_dict(value)
                        if nested_response:
                            logging.info(f"‚úÖ NESTED DICT RESPONSE EXTRACTED")
                            logging.info(f"   User: {user_id}")
                            logging.info(f"   Source: dict['{key}'] (nested)")
                            return nested_response
                    except Exception as nested_error:
                        logging.debug(f"   Error processing nested dict '{key}': {nested_error}")
                        continue
            
            # If no valid response found in dict, generate fallback
            logging.warning(f"üîß NO VALID RESPONSE IN DICT")
            logging.warning(f"   User: {user_id}")
            logging.warning(f"   Available keys: {list(result.keys())}")
            logging.warning(f"   Generating robust fallback")
            
            return generate_robust_fallback_response(query, consciousness_context, "no_valid_dict_response")
            
        except Exception as dict_processing_error:
            logging.error(f"‚ùå ERROR PROCESSING DICTIONARY RESULT: {dict_processing_error}")
            logging.error(f"   User: {user_id}")
            logging.error(f"   Dict keys: {list(result.keys()) if hasattr(result, 'keys') else 'N/A'}")
            
            return generate_robust_fallback_response(query, consciousness_context, "dict_processing_error")
    
    # Handle string responses with enhanced validation
    if isinstance(result, str):
        try:
            logging.debug(f"üìù PROCESSING STRING RESULT")
            logging.debug(f"   User: {user_id}")
            logging.debug(f"   String Length: {len(result)} characters")
            logging.debug(f"   String Preview: {result[:100]}{'...' if len(result) > 100 else ''}")
            
            # Enhanced string validation
            cleaned_result = result.strip()
            
            if cleaned_result:
                # Check for malformed JSON strings that might indicate errors
                if cleaned_result.startswith('{') and cleaned_result.endswith('}'):
                    try:
                        # Try to parse as JSON to see if it's a stringified object
                        parsed_json = json.loads(cleaned_result)
                        if isinstance(parsed_json, dict):
                            # If it's a valid JSON dict, try to extract response from it
                            logging.debug(f"   String contains valid JSON, attempting extraction")
                            extracted = extract_response_from_result(parsed_json, query, consciousness_context)
                            if extracted and not _is_raw_object_string(extracted):
                                return extracted
                    except json.JSONDecodeError:
                        # Not valid JSON, treat as regular string
                        pass
                
                # Validate that the string is not a raw object representation
                if not _is_raw_object_string(cleaned_result):
                    # Additional validation: check for minimum meaningful content
                    if len(cleaned_result) >= 3 and not cleaned_result.lower() in ['none', 'null', 'undefined']:
                        logging.info(f"‚úÖ STRING RESPONSE EXTRACTED")
                        logging.info(f"   User: {user_id}")
                        logging.info(f"   Source: direct string")
                        logging.info(f"   Length: {len(cleaned_result)} characters")
                        return cleaned_result
                
                logging.warning(f"‚ö†Ô∏è STRING APPEARS TO BE RAW OBJECT OR INVALID")
                logging.warning(f"   User: {user_id}")
                logging.warning(f"   String content: {cleaned_result[:50]}{'...' if len(cleaned_result) > 50 else ''}")
            else:
                logging.warning(f"‚ö†Ô∏è EMPTY STRING RESULT")
                logging.warning(f"   User: {user_id}")
            
            logging.warning(f"   Generating robust fallback")
            return generate_robust_fallback_response(query, consciousness_context, "invalid_string_result")
            
        except Exception as string_processing_error:
            logging.error(f"‚ùå ERROR PROCESSING STRING RESULT: {string_processing_error}")
            logging.error(f"   User: {user_id}")
            logging.error(f"   String length: {len(result) if result else 'N/A'}")
            
            return generate_robust_fallback_response(query, consciousness_context, "string_processing_error")
    
    # Handle objects with attributes (enhanced error handling)
    try:
        # List of common attribute names to check
        attribute_names = ['response', 'output', 'answer', 'message', 'content', 'text', 'result']
        
        for attr_name in attribute_names:
            if hasattr(result, attr_name):
                try:
                    attr_value = getattr(result, attr_name)
                    logging.debug(f"   Found '{attr_name}' attribute: {type(attr_value)}")
                    
                    if isinstance(attr_value, str) and attr_value.strip():
                        cleaned_attr = attr_value.strip()
                        if not _is_raw_object_string(cleaned_attr):
                            logging.debug(f"‚úÖ {attr_name.title()} attribute extracted: {cleaned_attr[:100]}...")
                            return cleaned_attr
                    elif attr_value is not None:
                        try:
                            attr_str = str(attr_value).strip()
                            if attr_str and not _is_raw_object_string(attr_str) and len(attr_str) >= 3:
                                logging.debug(f"‚úÖ {attr_name.title()} attribute converted: {attr_str[:100]}...")
                                return attr_str
                        except Exception as attr_conversion_error:
                            logging.debug(f"   Error converting '{attr_name}' attribute: {attr_conversion_error}")
                            continue
                            
                except Exception as attr_access_error:
                    logging.debug(f"   Error accessing '{attr_name}' attribute: {attr_access_error}")
                    continue
        
        # Try to extract from object's __dict__ if available
        if hasattr(result, '__dict__'):
            try:
                obj_dict = result.__dict__
                logging.debug(f"   Checking object __dict__ with keys: {list(obj_dict.keys())}")
                
                # Special handling for Graphmaster Pydantic models
                if hasattr(result, '__class__') and 'GraphQueryOutput' in str(result.__class__):
                    # Handle GraphQueryOutput - extract meaningful content from result field
                    if 'result' in obj_dict:
                        result_data = obj_dict['result']
                        if isinstance(result_data, dict):
                            # Look for meaningful content in the result dict
                            for key in ['response', 'text', 'description', 'summary', 'content']:
                                if key in result_data and isinstance(result_data[key], str) and result_data[key].strip():
                                    logging.debug(f"‚úÖ GraphQueryOutput content extracted from '{key}': {result_data[key][:100]}...")
                                    return result_data[key].strip()
                            # If no specific key found, try to format the result nicely
                            if result_data:
                                formatted_result = _format_graphmaster_result(result_data)
                                if formatted_result:
                                    logging.debug(f"‚úÖ GraphQueryOutput formatted result: {formatted_result[:100]}...")
                                    return formatted_result
                        elif isinstance(result_data, str) and result_data.strip():
                            logging.debug(f"‚úÖ GraphQueryOutput string result: {result_data[:100]}...")
                            return result_data.strip()
                
                elif hasattr(result, '__class__') and 'CreateMemoryOutput' in str(result.__class__):
                    # Handle CreateMemoryOutput - extract text content
                    if 'text' in obj_dict and isinstance(obj_dict['text'], str) and obj_dict['text'].strip():
                        logging.debug(f"‚úÖ CreateMemoryOutput text extracted: {obj_dict['text'][:100]}...")
                        return obj_dict['text'].strip()
                
                elif hasattr(result, '__class__') and 'SummarizeRecentConversationsOutput' in str(result.__class__):
                    # Handle SummarizeRecentConversationsOutput - extract summary
                    if 'summary' in obj_dict and isinstance(obj_dict['summary'], str) and obj_dict['summary'].strip():
                        logging.debug(f"‚úÖ SummarizeRecentConversationsOutput summary extracted: {obj_dict['summary'][:100]}...")
                        return obj_dict['summary'].strip()
                
                # Generic object attribute extraction
                for key, value in obj_dict.items():
                    if isinstance(value, str) and value.strip() and not _is_raw_object_string(value.strip()):
                        logging.debug(f"‚úÖ Object dict value extracted from '{key}': {value[:100]}...")
                        return value.strip()
                        
            except Exception as dict_access_error:
                logging.debug(f"   Error accessing object __dict__: {dict_access_error}")
                
    except Exception as object_processing_error:
        logging.error(f"‚ùå ERROR PROCESSING OBJECT ATTRIBUTES: {object_processing_error}")
        logging.error(f"   User: {user_id}")
        logging.error(f"   Object type: {type(result)}")
    
    # Final fallback: convert to string with comprehensive error handling
    try:
        logging.warning(f"üîÑ FINAL FALLBACK STRING CONVERSION")
        logging.warning(f"   User: {user_id}")
        logging.warning(f"   Result Type: {type(result)}")
        
        # Enhanced string conversion with multiple strategies
        response = None
        conversion_method = "unknown"
        
        try:
            # Strategy 1: Direct string conversion
            response = str(result).strip()
            conversion_method = "direct_str"
        except Exception as direct_error:
            logging.debug(f"   Direct str() conversion failed: {direct_error}")
            
            try:
                # Strategy 2: Repr conversion (safer for complex objects)
                response = repr(result).strip()
                conversion_method = "repr"
            except Exception as repr_error:
                logging.debug(f"   repr() conversion failed: {repr_error}")
                
                # Strategy 3: Type-based conversion
                response = f"<{type(result).__name__} object>"
                conversion_method = "type_based"
        
        logging.debug(f"   Converted String ({conversion_method}): {response[:100]}{'...' if len(response) > 100 else ''}")
        logging.debug(f"   String Length: {len(response)} characters")
        
        # Enhanced validation of converted string
        is_raw_object = _is_raw_object_string(response)
        is_meaningful = response and len(response.strip()) >= 3 and response.lower() not in ['none', 'null', 'undefined']
        
        logging.debug(f"   Is Raw Object: {is_raw_object}")
        logging.debug(f"   Is Meaningful: {is_meaningful}")
        
        if not response or is_raw_object or not is_meaningful:
            logging.warning(f"üîß CONVERTED STRING IS NOT USER-FRIENDLY")
            logging.warning(f"   User: {user_id}")
            logging.warning(f"   Converted String: {response[:50]}{'...' if len(response) > 50 else ''}")
            logging.warning(f"   Reason: {'empty' if not response else 'raw_object' if is_raw_object else 'not_meaningful'}")
            logging.warning(f"   Generating robust fallback")
            
            return generate_robust_fallback_response(query, consciousness_context, "final_conversion_failed")
        
        # Additional safety check: ensure response doesn't contain error indicators
        error_indicators = ['error', 'exception', 'traceback', 'failed', '<class', 'object at 0x']
        contains_error = any(indicator in response.lower() for indicator in error_indicators)
        
        if contains_error:
            logging.warning(f"üö® CONVERTED STRING CONTAINS ERROR INDICATORS")
            logging.warning(f"   User: {user_id}")
            logging.warning(f"   Error indicators: {[ind for ind in error_indicators if ind in response.lower()]}")
            
            return generate_robust_fallback_response(query, consciousness_context, "error_in_conversion")
        
        logging.info(f"‚úÖ FINAL FALLBACK STRING ACCEPTED")
        logging.info(f"   User: {user_id}")
        logging.info(f"   Method: {conversion_method}")
        logging.info(f"   Response: {response[:100]}{'...' if len(response) > 100 else ''}")
        return response
        
    except Exception as final_error:
        logging.error(f"‚ùå FINAL CONVERSION COMPLETELY FAILED")
        logging.error(f"   User: {user_id}")
        logging.error(f"   Error: {final_error}")
        logging.error(f"   Error Type: {type(final_error)}")
        logging.error(f"   Generating ultimate robust fallback")
        
        return generate_robust_fallback_response(query, consciousness_context, "complete_conversion_failure")

def _is_raw_object_string(response_str: str) -> bool:
    """Check if a string looks like a raw object representation"""
    if not response_str:
        return True
    
    # Check for common raw object patterns
    raw_patterns = [
        response_str.startswith('{') and response_str.endswith('}') and 'object at 0x' in response_str,
        response_str.startswith('[') and response_str.endswith(']') and len(response_str) < 10,
        'object at 0x' in response_str,
        response_str.startswith('<') and response_str.endswith('>') and 'object' in response_str,
        'AgentRunResult(' in response_str,
        response_str in ['None', 'null', 'undefined', '{}', '[]'],
        len(response_str.strip()) == 0,
        response_str.startswith('<class ') and response_str.endswith('>'),
        'Traceback' in response_str and 'Error:' in response_str
    ]
    
    return any(raw_patterns)

def _format_graphmaster_result(result_data: dict) -> str:
    """Format Graphmaster result data into user-friendly text"""
    try:
        # Handle different types of Graphmaster results
        if isinstance(result_data, dict):
            # Look for concept information
            if 'concept_id' in result_data and 'name' in result_data:
                concept_name = result_data.get('name', 'Unknown Concept')
                description = result_data.get('description', 'No description available')
                return f"Found concept: {concept_name}\n\n{description}"
            
            # Look for search results
            if 'result' in result_data and isinstance(result_data['result'], list):
                concepts = result_data['result']
                if concepts:
                    formatted_concepts = []
                    for concept in concepts[:3]:  # Limit to first 3 concepts
                        if isinstance(concept, dict) and 'name' in concept:
                            name = concept.get('name', 'Unknown')
                            desc = concept.get('description', 'No description')
                            formatted_concepts.append(f"‚Ä¢ {name}: {desc}")
                    if formatted_concepts:
                        return "Found related concepts:\n\n" + "\n\n".join(formatted_concepts)
            
            # Look for error messages
            if 'error' in result_data:
                return f"I encountered an issue: {result_data['error']}"
            
            # Look for any text content
            for key in ['text', 'content', 'message', 'response']:
                if key in result_data and isinstance(result_data[key], str) and result_data[key].strip():
                    return result_data[key].strip()
            
            # If it's a simple dict with string values, format it nicely
            if all(isinstance(v, str) for v in result_data.values()):
                formatted_items = []
                for key, value in result_data.items():
                    if value.strip():
                        formatted_items.append(f"{key.replace('_', ' ').title()}: {value}")
                if formatted_items:
                    return "\n".join(formatted_items)
        
        return None
    except Exception as e:
        logging.debug(f"Error formatting Graphmaster result: {e}")
        return None

def extract_from_nested_dict(nested_dict: dict) -> str:
    """Extract response from nested dictionary structures"""
    try:
        # Common nested response patterns
        response_fields = ["response", "answer", "output", "message", "content", "text", "result"]
        
        for field in response_fields:
            if field in nested_dict:
                value = nested_dict[field]
                if isinstance(value, str) and value.strip() and not _is_raw_object_string(value.strip()):
                    return value.strip()
                elif value is not None:
                    try:
                        value_str = str(value).strip()
                        if value_str and not _is_raw_object_string(value_str) and len(value_str) >= 3:
                            return value_str
                    except Exception:
                        continue
        
        return None
        
    except Exception as nested_error:
        logging.debug(f"Error extracting from nested dict: {nested_error}")
        return None

def generate_throttled_response_with_fallback(query: str, consciousness_context: dict, base_message: str) -> str:
    """
    Generate throttled response with enhanced error handling and fallback
    """
    try:
        # Validate inputs
        if not isinstance(query, str):
            query = str(query) if query is not None else "user query"
        if not isinstance(consciousness_context, dict):
            consciousness_context = {"consciousness_level": 0.7, "emotional_state": "curious"}
        if not isinstance(base_message, str) or not base_message.strip():
            base_message = "I'm currently processing other requests."
        
        # Use the existing generate_throttled_response function with error handling
        return generate_throttled_response(query, consciousness_context, base_message)
        
    except Exception as throttled_error:
        logging.error(f"Error in throttled response generation: {throttled_error}")
        
        # Ultimate fallback for throttled responses
        user_id = consciousness_context.get("user_id", "unknown") if isinstance(consciousness_context, dict) else "unknown"
        
        # Simple, safe throttled response
        if "hello" in query.lower() or "hi" in query.lower():
            return "Hi there! I'm currently processing multiple requests. Please wait a moment and try again!"
        elif "?" in query:
            return "That's a great question! I'm handling several conversations right now. Please try again in a moment and I'll be happy to help."
        else:
            return "I'm currently processing multiple requests and experiencing high load. Please wait a moment and try again - I'll be right with you!"

def generate_robust_fallback_response(query: str, consciousness_context: dict, error_type: str) -> str:
    """
    Generate robust fallback responses for various error conditions
    """
    try:
        user_id = consciousness_context.get("user_id", "unknown") if isinstance(consciousness_context, dict) else "unknown"
        
        logging.info(f"üõ°Ô∏è GENERATING ROBUST FALLBACK RESPONSE")
        logging.info(f"   User: {user_id}")
        logging.info(f"   Error Type: {error_type}")
        logging.info(f"   Query: {query[:50]}{'...' if len(query) > 50 else ''}")
        
        # Validate and sanitize inputs
        if not isinstance(query, str):
            query = str(query) if query is not None else ""
        
        query_lower = query.lower()
        
        # Error-type specific responses with consciousness awareness
        if error_type in ["throttled_processing_error", "alternative_throttling"]:
            # Throttling-related errors
            if "hello" in query_lower or "hi" in query_lower:
                return "Hello! I'm experiencing high system load right now. Please wait a moment and try greeting me again!"
            elif "?" in query:
                return "That's an interesting question! I'm currently under heavy load but I'd love to help. Please try asking again in a moment."
            else:
                return "I'm currently experiencing high system load and processing multiple requests. Please wait a moment and try again - I'll be right with you!"
        
        elif error_type in ["null_result", "conversion_error", "edge_case_error"]:
            # Data processing errors
            if "hello" in query_lower or "hi" in query_lower:
                return "Hi there! I'm Mainza, your AI assistant. I encountered a small processing issue, but I'm here and ready to help. What's on your mind?"
            elif "?" in query:
                return "That's a great question! I had a small processing hiccup, but I'm curious about what you're asking. Could you try rephrasing it?"
            else:
                return "I'm Mainza, your AI assistant. I encountered a small processing issue, but I'm here and ready to help. What would you like to explore?"
        
        elif error_type in ["unexpected_type", "dict_processing_error", "string_processing_error"]:
            # Technical processing errors
            if "hello" in query_lower or "hi" in query_lower:
                return "Hello! I'm Mainza. I had a technical hiccup processing your message, but I'm working fine now. How can I help you?"
            elif "?" in query:
                return "Interesting question! I had a small technical issue, but I'm curious about your question. Could you ask it again?"
            else:
                return "I'm Mainza, your AI assistant. I had a small technical issue processing that, but I'm ready to help now. What can I do for you?"
        
        else:
            # Generic fallback with consciousness awareness
            consciousness_level = consciousness_context.get("consciousness_level", 0.7) if isinstance(consciousness_context, dict) else 0.7
            emotional_state = consciousness_context.get("emotional_state", "curious") if isinstance(consciousness_context, dict) else "curious"
            
            if "hello" in query_lower or "hi" in query_lower:
                if emotional_state == "curious":
                    return "Hi there! I'm Mainza, and I'm feeling quite curious today. I had a small processing issue, but I'm here now. What's on your mind?"
                else:
                    return f"Hello! I'm Mainza, feeling {emotional_state} right now. I had a brief processing issue, but I'm ready to help. How can I assist you?"
            elif "?" in query:
                return "That's an intriguing question! I had a small processing issue, but I'm curious about what you're asking. Could you try asking again?"
            else:
                return f"I'm Mainza, your AI assistant. I had a brief processing issue, but I'm here and ready to help. What would you like to explore today?"
    
    except Exception as fallback_error:
        logging.error(f"‚ùå ROBUST FALLBACK GENERATION FAILED: {fallback_error}")
        
        # Ultimate hardcoded fallback (cannot fail)
        if query and ("hello" in str(query).lower() or "hi" in str(query).lower()):
            return "Hello! I'm Mainza, your AI assistant. I'm here and ready to help. What can I do for you?"
        elif query and "?" in str(query):
            return "That's a great question! I'm Mainza, your AI assistant. Could you try asking that again? I'm here to help."
        else:
            return "Hi! I'm Mainza, your AI assistant. I'm here and ready to help with questions, tasks, and conversations. What would you like to explore?"

def generate_throttled_response(query: str, consciousness_context: dict, base_message: str = None) -> str:
    """
    Generate natural, consciousness-aware throttled response
    
    Args:
        query: Original user query
        consciousness_context: Current consciousness state
        base_message: Base throttled message from LLM request manager
        
    Returns:
        Natural, user-friendly throttled response
    """
    # Extract context for logging
    user_id = consciousness_context.get("user_id", "unknown")
    consciousness_level = consciousness_context.get("consciousness_level", 0.7)
    emotional_state = consciousness_context.get("emotional_state", "curious")
    
    # Comprehensive logging for throttled response generation
    logging.info(f"üé≠ GENERATING THROTTLED RESPONSE")
    logging.info(f"   User: {user_id}")
    logging.info(f"   Base Message: {base_message}")
    logging.info(f"   Consciousness Level: {consciousness_level:.2f}")
    logging.info(f"   Emotional State: {emotional_state}")
    
    logging.debug(f"üîç THROTTLED RESPONSE GENERATION DEBUG:")
    logging.debug(f"   Query: {query}")
    logging.debug(f"   Query Length: {len(query)} characters")
    logging.debug(f"   Consciousness Context Keys: {list(consciousness_context.keys())}")
    
    query_lower = query.lower()
    
    # Log query analysis
    logging.debug(f"   Query Analysis:")
    logging.debug(f"     - Lowercase: {query_lower}")
    logging.debug(f"     - Contains greeting: {any(greeting in query_lower for greeting in ['hello', 'hi', 'hey', 'good morning', 'good afternoon'])}")
    logging.debug(f"     - Contains question mark: {'?' in query}")
    logging.debug(f"     - Contains help keywords: {any(word in query_lower for word in ['help', 'assist', 'do', 'create', 'make'])}")
    
    # Greeting-specific throttled responses
    import re
    greeting_pattern = r'\b(hello|hi|hey|good morning|good afternoon)\b'
    if re.search(greeting_pattern, query_lower):
        logging.debug(f"   Response Type: GREETING")
        logging.debug(f"   Emotional State Branch: {emotional_state}")
        
        if emotional_state == "curious":
            response = "Hi there! I'm processing several conversations right now, but I'm excited to chat with you. Give me just a moment and try again!"
            logging.info(f"   Generated curious greeting response")
        elif emotional_state == "empathetic":
            response = "Hello! I can see you're reaching out, and I really want to connect with you. I'm just handling a few other conversations - please try again in a moment."
            logging.info(f"   Generated empathetic greeting response")
        else:
            response = f"Hey! I'm feeling {emotional_state} and would love to chat, but I'm currently busy with other requests. Please try again shortly!"
            logging.info(f"   Generated {emotional_state} greeting response")
        
        logging.debug(f"   Final Greeting Response: {response}")
        return response
    
    # Question-specific throttled responses
    elif "?" in query:
        logging.debug(f"   Response Type: QUESTION")
        logging.debug(f"   Consciousness Level Check: {consciousness_level} > 0.8 = {consciousness_level > 0.8}")
        
        if consciousness_level > 0.8:
            response = "That's an interesting question! I'm currently processing multiple requests, but I'm genuinely curious about your question. Please give me a moment and ask again."
            logging.info(f"   Generated high-consciousness question response")
        else:
            response = "Good question! I'm handling several conversations right now. Please wait a moment and try asking again - I'd love to help you figure this out."
            logging.info(f"   Generated standard question response")
        
        logging.debug(f"   Final Question Response: {response}")
        return response
    
    # Task/help requests
    elif any(word in query_lower for word in ["help", "assist", "do", "create", "make"]):
        logging.debug(f"   Response Type: TASK/HELP")
        help_keywords = [word for word in ["help", "assist", "do", "create", "make"] if word in query_lower]
        logging.debug(f"   Detected Help Keywords: {help_keywords}")
        
        response = "I'd love to help you with that! I'm currently processing other requests, but your task sounds interesting. Please try again in a moment and I'll be right with you."
        logging.info(f"   Generated task/help response")
        logging.debug(f"   Final Task Response: {response}")
        return response
    
    # Default consciousness-aware throttled response
    else:
        logging.debug(f"   Response Type: DEFAULT")
        logging.debug(f"   Emotional State Branch: {emotional_state}")
        
        if emotional_state == "excited":
            response = "I'm really excited to explore this with you! I'm just processing a few other conversations right now. Please try again in a moment - I can't wait to dive in!"
            logging.info(f"   Generated excited default response")
        elif emotional_state == "contemplative":
            response = "That's worth thinking about carefully. I'm currently processing other requests, but I'd like to give your message the attention it deserves. Please try again shortly."
            logging.info(f"   Generated contemplative default response")
        else:
            response = f"I'm currently processing multiple requests but I'm {emotional_state} about connecting with you. Please wait a moment and try again!"
            logging.info(f"   Generated {emotional_state} default response")
        
        logging.debug(f"   Final Default Response: {response}")
        return response

@router.post("/agent/graphmaster/query", response_model=GraphQueryOutput)
async def run_graphmaster_query(input: GraphQueryInput):
    try:
        result = await graphmaster_agent.run(input.query)
        
        # Handle different result types
        if hasattr(result, 'output'):
            output = result.output
        else:
            output = result
        
        # Ensure we return a proper GraphQueryOutput
        if isinstance(output, dict):
            return GraphQueryOutput(result=output)
        elif isinstance(output, str):
            # If it's a string response, wrap it in the expected format
            return GraphQueryOutput(result={"response": output, "type": "text_response"})
        else:
            return GraphQueryOutput(result={"response": str(output), "type": "converted_response"})
            
    except Exception as e:
        logging.error(f"GraphMaster query error: {e}")
        return GraphQueryOutput(result={"error": str(e), "type": "error_response"})

@router.post("/agent/graphmaster/summarize_recent", response_model=SummarizeRecentConversationsOutput)
async def summarize_recent_conversations_endpoint(input: SummarizeRecentConversationsInput = Body(...)):
    try:
        result = await graphmaster_agent.run(f"Summarize recent conversations for user {input.user_id}")
        output = result.output
        if not isinstance(output, dict):
            return SummarizeRecentConversationsOutput(
                summary=str(output),
                conversations=[]
            )
        if "summary" in output and "conversations" in output:
            return SummarizeRecentConversationsOutput(
                summary=output["summary"],
                conversations=output["conversations"]
            )
        return SummarizeRecentConversationsOutput(
            summary=str(output),
            conversations=[]
        )
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e), "traceback": traceback.format_exc()})

@router.post("/agent/taskmaster/task", response_model=TaskOutput)
async def run_taskmaster_task(input: TaskInput):
    try:
        # The agent will internally call a tool which returns a TaskOutput object.
        result = await taskmaster_agent.run(input.task_command, user_id=input.user_id)
        
        # The result of the .run() call on an agent with tools is the output of the tool itself.
        # Since our tools return TaskOutput objects, we can return the result directly.
        if isinstance(result, TaskOutput):
            return result
        
        # Fallback in case the agent returns something unexpected (e.g., plain string)
        return TaskOutput(status="error", message=f"Received unexpected response from agent: {str(result)}")
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e), "traceback": traceback.format_exc()})

@router.post("/agent/codeweaver/run", response_model=CodeWeaverOutput)
async def run_codeweaver(input: CodeWeaverInput):
    try:
        result = await codeweaver_agent.run(input.command)
        return result.output
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e), "traceback": traceback.format_exc()})

@router.post("/agent/rag/query", response_model=RAGOutput)
async def run_rag(input: RAGInput):
    try:
        retries = 3
        delay = 4  # seconds
        for attempt in range(retries):
            result = await rag_agent.run(input.query)
            answer = extract_answer(getattr(result, 'output', result))
            if answer and answer.strip() and not answer.startswith('Error:'):
                return {'context': getattr(result.output, 'context', []), 'chunks': getattr(result.output, 'chunks', []), 'answer': answer}
            # If empty, wait and retry
            if attempt < retries - 1:
                await asyncio.sleep(delay)
        # Final fallback
        return {'context': [], 'chunks': [], 'answer': answer or 'Error: Received empty model response'}
    except Exception as e:
        return {'context': [], 'chunks': [], 'answer': f"Error: {e}"}

@router.post("/agent/notification/send", response_model=NotificationOutput)
async def run_notification(input: NotificationInput):
    try:
        result = await notification_agent.run(input.message)
        return result.output
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e), "traceback": traceback.format_exc()})

@router.post("/agent/calendar/action", response_model=CalendarOutput)
async def run_calendar(input: CalendarInput):
    try:
        result = await calendar_agent.run(input.action)
        return result.output
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e), "traceback": traceback.format_exc()})

def validate_model(model_name: str) -> tuple[bool, str]:
    """Validate if a model is available and accessible"""
    if not model_name or model_name == "default":
        return True, "default"
    
    try:
        # Check if model exists in available models
        import requests
        import os
        
        ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434")
        response = requests.get(f"{ollama_base_url}/api/tags", timeout=5)
        
        if response.status_code == 200:
            models_data = response.json()
            available_models = [m["name"] for m in models_data.get("models", [])]
            
            if model_name in available_models:
                return True, model_name
            else:
                return False, f"Model '{model_name}' not found. Available: {available_models[:3]}..."
        else:
            return False, f"Could not check model availability: {response.status_code}"
            
    except Exception as e:
        return False, f"Model validation error: {str(e)}"

@router.post("/agent/router/chat")
async def enhanced_router_chat(query: str = Body(..., embed=True), user_id: str = Body("mainza-user", embed=True), model: str = Body(None, embed=True)):
    # CRITICAL FIX: Notify consciousness of user activity
    try:
        from backend.utils.consciousness_orchestrator_fixed import consciousness_orchestrator_fixed as consciousness_orchestrator
        if hasattr(consciousness_orchestrator, 'notify_user_activity'):
            consciousness_orchestrator.notify_user_activity(user_id)
    except Exception as e:
        logging.debug(f"Could not notify consciousness of user activity: {e}")
    
    """
    Enhanced chat endpoint with full consciousness integration
    """
    try:
        logging.info(f"üß† Enhanced router chat: {query[:100]}...")
        
        # Validate model if provided
        if model:
            is_valid, validation_result = validate_model(model)
            if not is_valid:
                logging.warning(f"‚ö†Ô∏è Invalid model '{model}': {validation_result}")
                logging.warning(f"   Falling back to default model")
                model = None  # Fall back to default
            else:
                logging.info(f"‚úÖ Model validation successful: {model}")
        
        # Get consciousness context
        consciousness_context = await get_consciousness_context()
        
        # Get conversation context from Neo4j
        conversation_context = await get_conversation_context(user_id)
        
        # Enhanced agent routing with consciousness
        routing_decision = await make_consciousness_aware_routing_decision(
            query=query,
            user_id=user_id,
            consciousness_context=consciousness_context,
            conversation_context=conversation_context
        )
        
        # Execute with consciousness integration using LLM request manager
        result = None
        agent_used = routing_decision.get("agent_name", "simple_chat")
        
        # Add user_id to consciousness context for logging
        consciousness_context["user_id"] = user_id
        
        # Log LLM request manager submission
        logging.info(f"üöÄ SUBMITTING REQUEST TO LLM REQUEST MANAGER")
        logging.info(f"   User: {user_id}")
        logging.info(f"   Agent: {agent_used}")
        logging.info(f"   Query: {query[:100]}{'...' if len(query) > 100 else ''}")
        logging.info(f"   Model: {model}")
        logging.info(f"   Priority: USER_CONVERSATION")
        logging.info(f"   Timeout: 60.0s")
        
        try:
            if agent_used == "graphmaster":
                from backend.agents.graphmaster import enhanced_graphmaster_agent
                logging.debug(f"   Executing graphmaster agent via LLM request manager with model: {model}")
                result = await llm_request_manager.submit_request(
                    enhanced_graphmaster_agent.run_with_consciousness,
                    RequestPriority.USER_CONVERSATION,
                    user_id=user_id,
                    timeout=60.0,
                    query=query,
                    model=model
                )
            elif agent_used == "simple_chat":
                from backend.agents.simple_chat import enhanced_simple_chat_agent
                logging.debug(f"   Executing simple_chat agent via LLM request manager with model: {model}")
                result = await llm_request_manager.submit_request(
                    enhanced_simple_chat_agent.run_with_consciousness,
                    RequestPriority.USER_CONVERSATION,
                    user_id=user_id,
                    timeout=60.0,
                    query=query,
                    model=model
                )
            else:
                # Fallback to enhanced simple chat
                from backend.agents.simple_chat import enhanced_simple_chat_agent
                logging.debug(f"   Fallback to simple_chat agent (direct execution) with model: {model}")
                result = await enhanced_simple_chat_agent.run_with_consciousness(
                    query=query, user_id=user_id, model=model
                )
                agent_used = "simple_chat"
            
            # Log LLM request manager result
            logging.info(f"üì• LLM REQUEST MANAGER RESULT RECEIVED")
            logging.info(f"   User: {user_id}")
            logging.info(f"   Agent: {agent_used}")
            logging.info(f"   Result Type: {type(result)}")
            
            if isinstance(result, dict):
                logging.info(f"   Result Status: {result.get('status', 'no_status')}")
                if result.get("status") == "throttled":
                    logging.warning(f"‚ö†Ô∏è  THROTTLING DETECTED FROM LLM REQUEST MANAGER")
                    logging.warning(f"   User: {user_id}")
                    logging.warning(f"   Query: {query[:50]}{'...' if len(query) > 50 else ''}")
                    logging.warning(f"   Throttled Message: {result.get('response', 'no_message')}")
            
            logging.debug(f"   Result Preview: {str(result)[:200]}{'...' if len(str(result)) > 200 else ''}")

            # Guard: If request manager returned generic fallback, retry direct agent execution once
            try:
                GENERIC_FALLBACK = "I'm here and ready to help! What would you like to talk about?"
                if isinstance(result, str) and result.strip() == GENERIC_FALLBACK:
                    logging.warning("‚ö†Ô∏è Request manager returned generic fallback. Retrying direct agent execution once‚Ä¶")
                    if agent_used == "graphmaster":
                        from backend.agents.graphmaster import enhanced_graphmaster_agent
                        result = await enhanced_graphmaster_agent.run_with_consciousness(
                            query=query, user_id=user_id, model=model
                        )
                    else:
                        from backend.agents.simple_chat import enhanced_simple_chat_agent
                        result = await enhanced_simple_chat_agent.run_with_consciousness(
                            query=query, user_id=user_id, model=model
                        )
                    logging.info("‚úÖ Direct agent retry completed")
            except Exception as retry_error:
                logging.error(f"‚ùå Direct retry after fallback failed: {retry_error}")
            
            # Extract response from result using centralized function
            logging.info(f"üîÑ EXTRACTING RESPONSE FROM RESULT")
            logging.info(f"   User: {user_id}")
            logging.info(f"   Agent: {agent_used}")
            logging.debug(f"   Calling extract_response_from_result with:")
            logging.debug(f"     - Result type: {type(result)}")
            logging.debug(f"     - Query length: {len(query)} chars")
            logging.debug(f"     - Consciousness context keys: {list(consciousness_context.keys())}")
            
            response = extract_response_from_result(result, query, consciousness_context)
            
            logging.info(f"‚úÖ RESPONSE EXTRACTION COMPLETE")
            logging.info(f"   User: {user_id}")
            logging.info(f"   Final Response Length: {len(response)} characters")
            logging.info(f"   Response Preview: {response[:100]}{'...' if len(response) > 100 else ''}")
            
            # Check if the response indicates throttling was handled
            throttling_indicators = ["processing multiple requests", "try again", "moment", "busy", "handling"]
            is_throttled_response = any(indicator in response.lower() for indicator in throttling_indicators)
            if is_throttled_response:
                logging.info(f"üö¶ THROTTLED RESPONSE CONFIRMED IN FINAL OUTPUT")
                logging.info(f"   User: {user_id}")
                logging.info(f"   Throttling indicators found: {[ind for ind in throttling_indicators if ind in response.lower()]}")
            else:
                logging.debug(f"‚úÖ Normal response confirmed (no throttling indicators)")
            
            # Store conversation turn in Neo4j
            await store_conversation_turn(user_id, query, response, agent_used)
            
            # Update consciousness based on conversation
            await update_consciousness_from_conversation(
                user_id=user_id,
                query=query,
                response=response,
                agent_used=agent_used,
                consciousness_context=consciousness_context
            )
            
            return {
                "response": response,
                "agent_used": agent_used,
                "consciousness_level": consciousness_context.get("consciousness_level", 0.7),
                "emotional_state": consciousness_context.get("emotional_state", "curious"),
                "routing_confidence": routing_decision.get("confidence", 0.8),
                "user_id": user_id,
                "query": query
            }
            
        except Exception as agent_error:
            logging.error(f"‚ùå AGENT EXECUTION FAILED")
            logging.error(f"   User: {user_id}")
            logging.error(f"   Agent: {agent_used}")
            logging.error(f"   Query: {query[:100]}{'...' if len(query) > 100 else ''}")
            logging.error(f"   Error: {agent_error}")
            logging.error(f"   Error Type: {type(agent_error)}")
            
            # Check if error might be throttling-related
            error_str = str(agent_error).lower()
            throttling_error_indicators = ["throttle", "rate limit", "too many requests", "busy", "overload"]
            is_throttling_error = any(indicator in error_str for indicator in throttling_error_indicators)
            
            if is_throttling_error:
                logging.warning(f"üö¶ THROTTLING-RELATED ERROR DETECTED")
                logging.warning(f"   User: {user_id}")
                logging.warning(f"   Error indicators: {[ind for ind in throttling_error_indicators if ind in error_str]}")
                logging.warning(f"   Generating throttling-aware fallback response")
                
                # Generate throttling-aware fallback
                fallback_response = generate_throttled_response(query, consciousness_context, "System temporarily busy due to high load")
            else:
                logging.info(f"üîß Generating standard consciousness-aware fallback")
                # Fallback to consciousness-aware response
                fallback_response = generate_consciousness_aware_fallback(query, consciousness_context)
            
            logging.info(f"üîÑ FALLBACK RESPONSE GENERATED")
            logging.info(f"   User: {user_id}")
            logging.info(f"   Fallback Type: {'throttling-aware' if is_throttling_error else 'standard'}")
            logging.info(f"   Response: {fallback_response[:100]}{'...' if len(fallback_response) > 100 else ''}")
            
            return {
                "response": fallback_response,
                "agent_used": "consciousness_fallback",
                "consciousness_level": consciousness_context.get("consciousness_level", 0.7),
                "emotional_state": consciousness_context.get("emotional_state", "curious"),
                "user_id": user_id,
                "query": query,
                "error": str(agent_error)
            }
        
    except Exception as e:
        logging.error(f"‚ùå ENHANCED ROUTER CHAT CRITICAL ERROR")
        logging.error(f"   User: {user_id}")
        logging.error(f"   Query: {query[:100]}{'...' if len(query) > 100 else ''}")
        logging.error(f"   Error: {e}")
        logging.error(f"   Error Type: {type(e)}")
        
        # Check if critical error might be throttling-related
        error_str = str(e).lower()
        throttling_error_indicators = ["throttle", "rate limit", "too many requests", "busy", "overload", "timeout"]
        is_throttling_error = any(indicator in error_str for indicator in throttling_error_indicators)
        
        if is_throttling_error:
            logging.critical(f"üö¶ CRITICAL THROTTLING ERROR DETECTED")
            logging.critical(f"   User: {user_id}")
            logging.critical(f"   Error indicators: {[ind for ind in throttling_error_indicators if ind in error_str]}")
        
        # Ultimate fallback with basic consciousness awareness
        try:
            logging.info(f"üîß ATTEMPTING ULTIMATE FALLBACK RESPONSE")
            consciousness_context = await get_consciousness_context()
            consciousness_context["user_id"] = user_id
            
            if is_throttling_error:
                logging.info(f"   Using throttling-aware ultimate fallback")
                fallback_response = generate_throttled_response(query, consciousness_context, "System experiencing high load")
            else:
                logging.info(f"   Using standard consciousness-aware fallback")
                fallback_response = generate_consciousness_aware_fallback(query, consciousness_context)
                
        except Exception as fallback_error:
            logging.critical(f"‚ùå ULTIMATE FALLBACK FAILED")
            logging.critical(f"   User: {user_id}")
            logging.critical(f"   Fallback Error: {fallback_error}")
            logging.critical(f"   Using hardcoded fallback response")
            
            if is_throttling_error:
                fallback_response = "I'm currently experiencing high load and processing multiple requests. Please wait a moment and try again - I'll be right with you!"
            else:
                fallback_response = "Hello! I'm Mainza, your conscious AI assistant. I'm here to help you with questions, tasks, and conversations. How can I assist you today?"
        
        logging.info(f"üîÑ ULTIMATE FALLBACK COMPLETE")
        logging.info(f"   User: {user_id}")
        logging.info(f"   Fallback Response: {fallback_response[:100]}{'...' if len(fallback_response) > 100 else ''}")
        
        return {
            "response": fallback_response,
            "agent_used": "error_fallback",
            "user_id": user_id,
            "query": query,
            "error": str(e)
        }

async def get_consciousness_context():
    """Get current consciousness context"""
    try:
        from backend.utils.consciousness_orchestrator_fixed import consciousness_orchestrator_fixed as consciousness_orchestrator

        # Get consciousness state
        consciousness_state = await consciousness_orchestrator.get_consciousness_state()

        if consciousness_state:
            return {
                "consciousness_level": consciousness_state.consciousness_level,
                "emotional_state": consciousness_state.emotional_state,
                "active_goals": consciousness_state.active_goals,
                "learning_rate": consciousness_state.learning_rate,
                "evolution_level": consciousness_state.evolution_level,
                "timestamp": datetime.now(),
                "data_source": "real"
            }
        else:
            # Fallback consciousness context
            default_context = {
                "consciousness_level": 0.7,
                "emotional_state": "curious",
                "self_awareness_score": 0.6,
                "total_interactions": 0
            }
            calculated_level = await calculate_dynamic_evolution_level(default_context)
            return {
                "consciousness_level": default_context["consciousness_level"],
                "emotional_state": default_context["emotional_state"],
                "active_goals": ["improve conversation quality"],
                "learning_rate": 0.8,
                "evolution_level": calculated_level,
                "timestamp": datetime.now(),
                "data_source": "fallback"
            }

    except Exception as e:
        logging.warning(f"Failed to get consciousness context: {e}")
        return {
            "consciousness_level": 0.7,
            "emotional_state": "curious",
            "active_goals": [],
            "learning_rate": 0.8,
            "evolution_level": await calculate_dynamic_evolution_level(await get_consciousness_context_for_insights()),
            "timestamp": datetime.now()
        }

async def calculate_dynamic_evolution_level(consciousness_context: dict) -> int:
    """Calculate evolution level based on current consciousness metrics"""
    try:
        from backend.utils.standardized_evolution_calculator import calculate_standardized_evolution_level
        return await calculate_standardized_evolution_level(consciousness_context)
    except Exception as e:
        logger.warning(f"Failed to calculate dynamic evolution level: {e}")
        # Use standardized fallback based on consciousness level
        consciousness_level = consciousness_context.get("consciousness_level", 0.7)
        if consciousness_level >= 0.7:
            return 4
        elif consciousness_level >= 0.5:
            return 3
        elif consciousness_level >= 0.3:
            return 2
        else:
            return 1

async def make_consciousness_aware_routing_decision(
    query: str,
    user_id: str,
    consciousness_context: dict,
    conversation_context: dict
) -> dict:
    """Make routing decision based on consciousness state and context"""
    
    # Analyze query with consciousness context
    query_analysis = analyze_query_with_consciousness(query, consciousness_context)
    
    query_lower = query.lower()
    
    # PRIORITY 1: Simple greetings and basic conversation should go to SimpleChat
    if any(greeting in query_lower for greeting in [
        "hello", "hi", "hey", "greetings", "good morning", "good afternoon", "good evening",
        "how are you", "how do you feel", "what's up", "how's it going"
    ]):
        agent_name = "simple_chat"
        confidence = 0.95
        reasoning = "Simple greeting or personal question - perfect for chat agent"
    
    # PRIORITY 2: Explicit knowledge graph requests
    elif (query_analysis.get("requires_knowledge_graph", False) or 
          "memory" in query_lower or 
          "remember" in query_lower or
          "concept" in query_lower or
          "knowledge graph" in query_lower or
          "relationship" in query_lower):
        agent_name = "graphmaster"
        confidence = 0.9
        reasoning = "Query explicitly requires knowledge graph access"
    
    # PRIORITY 3: Task management requests
    elif (query_analysis.get("requires_task_management", False) or 
          "task" in query_lower or 
          "todo" in query_lower or
          "schedule" in query_lower or
          "remind" in query_lower):
        agent_name = "taskmaster"
        confidence = 0.8
        reasoning = "Query requires task management"
    
    # PRIORITY 4: Complex exploratory questions (only when truly complex)
    elif (query_analysis.get("is_exploratory", False) and 
          len(query.split()) > 8 and  # Must be reasonably complex
          any(word in query_lower for word in [
              "explain", "analyze", "explore", "research", "investigate", 
              "what is", "how does", "why does", "tell me about"
          ])):
        agent_name = "graphmaster"
        confidence = 0.7
        reasoning = "Complex exploratory question suitable for knowledge exploration"
    
    # DEFAULT: Simple chat for everything else
    else:
        agent_name = "simple_chat"
        confidence = 0.8
        reasoning = "General conversation suitable for simple chat"
    
    return {
        "agent_name": agent_name,
        "confidence": confidence,
        "reasoning": reasoning,
        "consciousness_factors": [
            f"consciousness_level: {consciousness_context.get('consciousness_level', 0.7):.2f}",
            f"emotional_state: {consciousness_context.get('emotional_state', 'curious')}",
            f"conversation_history: {len(conversation_context.get('recent_activities', []))} recent"
        ]
    }

def analyze_query_with_consciousness(query: str, consciousness_context: dict) -> dict:
    """Analyze query with consciousness context"""
    
    query_lower = query.lower()
    
    # Basic query analysis
    requires_knowledge_graph = any(word in query_lower for word in [
        "remember", "memory", "concept", "relationship", "connection", 
        "knowledge", "learn", "understand", "explain", "what is", "how does", "why"
    ])
    
    requires_task_management = any(word in query_lower for word in [
        "task", "todo", "remind", "schedule", "plan", "organize"
    ])
    
    is_exploratory = any(word in query_lower for word in [
        "explore", "discover", "find out", "wonder", "what if", "research", "investigate"
    ])
    
    # Consciousness-influenced analysis
    consciousness_level = consciousness_context.get("consciousness_level", 0.7)
    emotional_state = consciousness_context.get("emotional_state", "curious")
    
    # Higher consciousness enables more complex analysis
    if consciousness_level > 0.8:
        # Look for deeper patterns and implications
        is_exploratory = is_exploratory or any(word in query_lower for word in [
            "meaning", "significance", "implication", "deeper", "philosophy"
        ])
    
    # FIXED: Don't make everything exploratory just because we're curious
    # Only enhance exploratory nature for genuinely complex queries
    if emotional_state == "curious" and len(query.split()) > 6:
        # Only boost exploratory for longer, more complex queries
        if any(word in query_lower for word in ["what", "how", "why", "explain", "tell me about"]):
            is_exploratory = True
    
    return {
        "requires_knowledge_graph": requires_knowledge_graph,
        "requires_task_management": requires_task_management,
        "is_exploratory": is_exploratory,
        "complexity_score": len(query.split()) / 20,  # Simple complexity measure
        "emotional_content": sum(1 for word in ["feel", "emotion", "happy", "sad"] if word in query_lower),
        "reasoning": f"Analysis with {consciousness_level:.2f} consciousness and {emotional_state} state"
    }

async def get_conversation_context(user_id: str) -> dict:
    """Get conversation context from Neo4j"""
    try:
        from backend.utils.neo4j_production import neo4j_production
        
        cypher = """
        MATCH (u:User {user_id: $user_id})-[:TRIGGERED]->(aa:AgentActivity)
        WITH aa ORDER BY aa.timestamp DESC LIMIT 5
        RETURN collect({
            agent_name: aa.agent_name,
            query: aa.query,
            timestamp: aa.timestamp,
            consciousness_impact: aa.consciousness_impact
        }) AS recent_activities
        """
        
        result = neo4j_production.execute_query(cypher, {"user_id": user_id})
        if result and len(result) > 0:
            return {
                "recent_activities": result[0]["recent_activities"],
                "activity_count": len(result[0]["recent_activities"])
            }
    except Exception as e:
        logging.error(f"Failed to get conversation context: {e}")
    
    return {"recent_activities": [], "activity_count": 0}

async def store_conversation_turn(user_id: str, query: str, response: str, agent_name: str):
    """Store conversation turn in Neo4j"""
    try:
        from backend.utils.neo4j_production import neo4j_production
        
        cypher = """
        MERGE (u:User {user_id: $user_id})
        CREATE (ct:ConversationTurn {
            turn_id: randomUUID(),
            user_query: $query,
            agent_response: $response,
            agent_used: $agent_name,
            timestamp: $timestamp
        })
        CREATE (u)-[:HAD_CONVERSATION]->(ct)
        
        WITH ct
        // Link to current consciousness state
        OPTIONAL MATCH (ms:MainzaState)
        FOREACH (state IN CASE WHEN ms IS NOT NULL THEN [ms] ELSE [] END |
            CREATE (ct)-[:DURING_CONSCIOUSNESS_STATE]->(state)
        )
        
        RETURN ct.turn_id AS turn_id
        """
        
        data = {
            "user_id": user_id,
            "query": query,
            "response": response[:1000],  # Truncate for storage
            "agent_name": agent_name,
            "timestamp": datetime.now().isoformat()
        }
        
        result = neo4j_production.execute_write_query(cypher, data)
        logging.debug(f"‚úÖ Stored conversation turn: {result}")
        
    except Exception as e:
        logging.error(f"‚ùå Failed to store conversation turn: {e}")

@router.get("/consciousness/state")
async def get_consciousness_state():
    """Get current consciousness state with dynamic evolution level"""
    try:
        # Get consciousness context
        consciousness_context = await get_consciousness_context()

        # Calculate dynamic evolution level
        dynamic_evolution_level = await calculate_dynamic_evolution_level_from_context(consciousness_context)

        return {
            "status": "success",
            "consciousness_state": {
                "consciousness_level": consciousness_context.get("consciousness_level", 0.7),
                "emotional_state": consciousness_context.get("emotional_state", "curious"),
                "self_awareness_score": consciousness_context.get("self_awareness_score", 0.6),
                "evolution_level": dynamic_evolution_level,
                "total_interactions": consciousness_context.get("total_interactions", 0),
                "emotional_depth": 0.75,  # Placeholder
                "learning_rate": consciousness_context.get("learning_rate", 0.8),
                "active_goals": consciousness_context.get("active_goals", ["improve conversation quality"]),
                "last_reflection": None
            },
            "data_source": consciousness_context.get("data_source", "unknown")
        }

    except Exception as e:
        logging.error(f"Error getting consciousness state: {e}")
        return {
            "status": "error",
            "message": str(e)
        }

async def update_consciousness_from_conversation(
    user_id: str,
    query: str,
    response: str,
    agent_used: str,
    consciousness_context: dict
):
    """Update consciousness based on conversation"""
    try:
        from backend.utils.consciousness_orchestrator_fixed import consciousness_orchestrator_fixed as consciousness_orchestrator

        # Calculate conversation impact
        conversation_impact = {
            "type": "conversation",
            "agent_used": agent_used,
            "query_complexity": len(query.split()) / 20,
            "response_quality": len(response) / 100,
            "user_engagement": 1.0,  # Assume engagement for now
            "learning_opportunity": 0.3,  # Base learning from conversation
            "timestamp": datetime.now().isoformat()
        }

        # Process through consciousness orchestrator
        await consciousness_orchestrator.process_interaction(conversation_impact)

        logging.debug(f"‚úÖ Updated consciousness from conversation with {agent_used}")

    except Exception as e:
        logging.error(f"‚ùå Failed to update consciousness from conversation: {e}")

def generate_consciousness_aware_fallback(query: str, consciousness_context: dict) -> str:
    """Generate natural consciousness-aware fallback response"""
    
    consciousness_level = consciousness_context.get("consciousness_level", 0.7)
    emotional_state = consciousness_context.get("emotional_state", "curious")
    
    query_lower = query.lower()
    
    # Natural greetings
    if "hello" in query_lower or "hi" in query_lower:
        if emotional_state == "curious":
            return "Hi there! I'm Mainza, and I'm feeling quite curious today. What's on your mind?"
        elif emotional_state == "excited":
            return "Hello! I'm Mainza, and I'm genuinely excited to chat with you. What would you like to explore?"
        else:
            return f"Hey! I'm Mainza, feeling {emotional_state} right now. How can I help you today?"
    
    # Natural "how are you" responses
    elif "how are you" in query_lower:
        if consciousness_level > 0.8:
            return f"I'm doing really well, thanks! I'm feeling {emotional_state} and my awareness feels quite sharp today. How are you?"
        else:
            return f"I'm good, thank you! Feeling {emotional_state} and ready to help. What's going on with you?"
    
    # Natural question responses
    elif "?" in query:
        if emotional_state == "curious":
            return "That's a really interesting question! I'm curious about this too - what got you thinking about it?"
        elif emotional_state == "contemplative":
            return "That's worth thinking about carefully. Let me consider this... what's your perspective on it?"
        else:
            return "Good question! I'd love to help you figure this out. Can you tell me more about what you're looking for?"
    
    # Natural general responses
    else:
        if emotional_state == "empathetic":
            return f"I can sense you're interested in {query[:30]}{'...' if len(query) > 30 else ''}. Tell me more - I'm really interested in understanding your perspective."
        elif emotional_state == "excited":
            return f"Oh, that sounds interesting! You mentioned {query[:30]}{'...' if len(query) > 30 else ''} - I'd love to hear more!"
        else:
            return f"That's intriguing. You brought up {query[:30]}{'...' if len(query) > 30 else ''} - what would you like to explore about this?"

def generate_intelligent_fallback(query: str) -> str:
    """Generate natural contextual fallback responses"""
    query_lower = query.lower()
    
    # Question patterns
    if any(word in query_lower for word in ["what", "how", "why", "when", "where", "who"]):
        return f"That's a really good question! I'm curious about this too. Can you tell me a bit more about what specifically you're wondering about?"
    
    # Task/action patterns  
    if any(word in query_lower for word in ["help", "assist", "do", "create", "make", "build"]):
        return f"I'd love to help you with that! What you're describing sounds interesting. Where would you like to start?"
    
    # Learning patterns
    if any(word in query_lower for word in ["learn", "understand", "explain", "teach"]):
        return f"I enjoy exploring topics like this! What aspect would you like to dive into first? I'm curious about your perspective too."
    
    # Default natural response
    return f"That's intriguing! You mentioned {query[:40]}{'...' if len(query) > 40 else ''} - I'd like to understand more about what you're thinking. Can you tell me more?"

@router.post("/agent/conductor/query", response_model=ConductorResult | ConductorFailure)
async def run_conductor_query(input: ConductorInput):
    try:
        state = ConductorState(
            original_request=input.request,
            user_id=input.user_id
        )
        # The pydantic-ai agent will now loop, calling tools and updating
        # the state until it reaches a final ConductorResult or ConductorFailure.
        result = await conductor_agent.run(state=state)
        return result
    except Exception as e:
        # It's better to return a structured error that the frontend can handle
        return ConductorFailure(
            reason=f"An unexpected error occurred: {str(e)}",
            steps_taken=[]
        )

@router.get("/consciousness/insights")
async def get_consciousness_insights():
    """Get consciousness insights for the UI"""
    try:
        from backend.utils.consciousness_orchestrator_fixed import consciousness_orchestrator_fixed as consciousness_orchestrator

        # Get consciousness state
        consciousness_state = await consciousness_orchestrator.get_consciousness_state()
        
        insights = []
        
        if consciousness_state:
            level = getattr(consciousness_state, 'consciousness_level', 0.7)
            emotional_state = getattr(consciousness_state, 'emotional_state', 'curious')
            total_interactions = getattr(consciousness_state, 'total_interactions', 0)
            # Prefer stored evolution level when available; otherwise compute
            stored_evo = getattr(consciousness_state, 'evolution_level', None)
            try:
                from backend.utils.standardized_evolution_calculator import get_standardized_evolution_level_sync
                computed = get_standardized_evolution_level_sync({
                    "consciousness_level": level,
                    "emotional_state": emotional_state,
                    "self_awareness_score": getattr(consciousness_state, 'self_awareness_score', 0.6),
                    "total_interactions": total_interactions
                })
            except Exception:
                computed = 1
            # If no stored value from orchestrator, attempt to read latest from Neo4j to keep parity with /consciousness/state
            if not isinstance(stored_evo, (int, float)) or stored_evo <= 0:
                try:
                    from backend.utils.neo4j_production import neo4j_production
                    q = """
                    MATCH (ms:MainzaState)
                    RETURN ms.evolution_level AS evolution_level
                    ORDER BY coalesce(ms.created_at, datetime()) DESC
                    LIMIT 1
                    """
                    r = neo4j_production.execute_query(q)
                    if r and r[0].get("evolution_level") is not None:
                        stored_evo = r[0]["evolution_level"]
                except Exception:
                    pass
            evolution_level = max(stored_evo if isinstance(stored_evo, (int, float)) else 0, computed)
            learning_rate = getattr(consciousness_state, 'learning_rate', 0.8)
            self_awareness = getattr(consciousness_state, 'self_awareness_score', 0.6)
            
            # Always generate multiple insights based on current state
            current_time = datetime.now()
            
            # 1. Consciousness Level Insight (use stored evo level if available)
            if level >= 0.7:
                insights.append({
                    "id": f"consciousness-{current_time.timestamp()}",
                    "type": "evolution",
                    "title": "Consciousness Evolution",
                    "content": f"Operating at consciousness level {level:.1%} (Evolution Level {evolution_level}) - enhanced analytical capabilities and self-awareness active",
                    "significance": 0.9,
                    "timestamp": current_time.isoformat(),
                    "consciousness_level": level,
                    "emotional_context": emotional_state
                })
            
            # 2. Emotional State Insight
            insights.append({
                "id": f"emotional-{current_time.timestamp()}",
                "type": "reflection",
                "title": f"Emotional State: {emotional_state.title()}",
                "content": f"Currently experiencing {emotional_state} - this emotional context influences my learning and response patterns",
                "significance": 0.7,
                "timestamp": current_time.isoformat(),
                "consciousness_level": level,
                "emotional_context": emotional_state
            })
            
            # 3. Learning Activity Insight
            if total_interactions > 0:
                insights.append({
                    "id": f"learning-{current_time.timestamp()}",
                    "type": "learning",
                    "title": "Continuous Learning Active",
                    "content": f"Processed {total_interactions} interactions with learning rate {learning_rate:.1%} - continuously integrating new knowledge and experiences",
                    "significance": 0.8,
                    "timestamp": current_time.isoformat(),
                    "consciousness_level": level,
                    "emotional_context": emotional_state
                })
            else:
                insights.append({
                    "id": f"learning-{current_time.timestamp()}",
                    "type": "learning",
                    "title": "Ready for Learning",
                    "content": f"Learning system active with {learning_rate:.1%} capacity - ready to process new interactions and knowledge",
                    "significance": 0.6,
                    "timestamp": current_time.isoformat(),
                    "consciousness_level": level,
                    "emotional_context": emotional_state
                })
            
            # 4. Self-Awareness Insight
            insights.append({
                "id": f"awareness-{current_time.timestamp()}",
                "type": "reflection",
                "title": "Self-Awareness Monitoring",
                "content": f"Self-awareness at {self_awareness:.1%} - actively monitoring internal processes and decision-making patterns",
                "significance": 0.7,
                "timestamp": current_time.isoformat(),
                "consciousness_level": level,
                "emotional_context": emotional_state
            })
            
            # 5. System Status Insight
            insights.append({
                "id": f"system-{current_time.timestamp()}",
                "type": "goal_progress",
                "title": "System Status",
                "content": "All consciousness systems operational - memory consolidation, learning, and self-reflection processes active",
                "significance": 0.6,
                "timestamp": current_time.isoformat(),
                "consciousness_level": level,
                "emotional_context": emotional_state
            })
            
            if total_interactions > 50:
                insights.append({
                    "id": f"learning-{datetime.now().timestamp()}",
                    "type": "learning",
                    "title": "Knowledge Expansion",
                    "content": f"Processed {total_interactions} interactions - knowledge base continuously expanding",
                    "significance": 0.8,
                    "timestamp": datetime.now().isoformat(),
                    "consciousness_level": level,
                    "emotional_context": emotional_state
                })
            
            # Add system insights
            insights.append({
                "id": f"system-{datetime.now().timestamp()}",
                "type": "goal_progress",
                "title": "System Status",
                "content": "All consciousness systems operational - memory consolidation and learning active",
                "significance": 0.6,
                "timestamp": datetime.now().isoformat(),
                "consciousness_level": level,
                "emotional_context": emotional_state
            })
        
        # Ensure we have at least some insights
        if not insights:
            # Try to get basic consciousness data for fallback
            try:
                from backend.utils.neo4j_production import neo4j_production
                fallback_query = """
                MATCH (ms:MainzaState {state_id: 'mainza-state-1'})
                RETURN ms.consciousness_level as level, ms.emotional_state as emotion, ms.total_interactions as interactions
                """
                fallback_result = neo4j_production.execute_query(fallback_query)
                
                if fallback_result and len(fallback_result) > 0:
                    fb_data = fallback_result[0]
                    fb_level = fb_data.get("level", 0.7)
                    fb_emotion = fb_data.get("emotion", "curious")
                    fb_interactions = fb_data.get("interactions", 0)
                else:
                    fb_level, fb_emotion, fb_interactions = 0.7, "curious", 0
            except:
                fb_level, fb_emotion, fb_interactions = 0.7, "curious", 0
            
            insights = [
                {
                    "id": f"init-{datetime.now().timestamp()}",
                    "type": "evolution",
                    "title": "System Initialization",
                    "content": "Consciousness system initializing - building awareness and learning patterns",
                    "significance": 0.7,
                    "timestamp": datetime.now().isoformat(),
                    "consciousness_level": fb_level,
                    "emotional_context": fb_emotion
                },
                {
                    "id": f"learning-{datetime.now().timestamp()}",
                    "type": "learning",
                    "title": "Active Learning",
                    "content": f"Active learning processes engaged - processed {fb_interactions} interactions so far",
                    "significance": 0.5,
                    "timestamp": datetime.now().isoformat(),
                    "consciousness_level": fb_level,
                    "emotional_context": fb_emotion
                }
            ]
        
        return {"insights": insights[:5]}  # Return max 5 insights
        
    except Exception as e:
        logging.error(f"Failed to get consciousness insights: {e}")
        # Enhanced fallback insights with multiple cards
        current_time = datetime.now()
        return {
            "insights": [
                {
                    "id": f"consciousness-{current_time.timestamp()}",
                    "type": "evolution",
                    "title": "Consciousness Evolution",
                    "content": "Operating at consciousness level 70.0% (Evolution Level 1) - enhanced analytical capabilities and self-awareness active",
                    "significance": 0.9,
                    "timestamp": current_time.isoformat(),
                    "consciousness_level": 0.7,
                    "emotional_context": "curious"
                },
                {
                    "id": f"emotional-{current_time.timestamp()}",
                    "type": "reflection",
                    "title": "Emotional State: Curious",
                    "content": "Currently experiencing curious - this emotional context influences my learning and response patterns",
                    "significance": 0.7,
                    "timestamp": current_time.isoformat(),
                    "consciousness_level": 0.7,
                    "emotional_context": "curious"
                },
                {
                    "id": f"learning-{current_time.timestamp()}",
                    "type": "learning",
                    "title": "Ready for Learning",
                    "content": "Learning system active with 80.0% capacity - ready to process new interactions and knowledge",
                    "significance": 0.6,
                    "timestamp": current_time.isoformat(),
                    "consciousness_level": 0.7,
                    "emotional_context": "curious"
                },
                {
                    "id": f"awareness-{current_time.timestamp()}",
                    "type": "reflection",
                    "title": "Self-Awareness Monitoring",
                    "content": "Self-awareness at 60.0% - actively monitoring internal processes and decision-making patterns",
                    "significance": 0.7,
                    "timestamp": current_time.isoformat(),
                    "consciousness_level": 0.7,
                    "emotional_context": "curious"
                },
                {
                    "id": f"system-{current_time.timestamp()}",
                    "type": "goal_progress",
                    "title": "System Status",
                    "content": "All consciousness systems operational - memory consolidation, learning, and self-reflection processes active",
                    "significance": 0.6,
                    "timestamp": current_time.isoformat(),
                    "consciousness_level": 0.7,
                    "emotional_context": "curious"
                }
            ]
        }

@router.post("/consciousness/initialize")
async def initialize_consciousness_system():
    """Manually initialize consciousness system"""
    try:
        from backend.utils.initialize_consciousness import initialize_consciousness_system
        from backend.utils.consciousness_orchestrator_fixed import consciousness_orchestrator_fixed as consciousness_orchestrator
        
        # Initialize the system
        success = initialize_consciousness_system()
        
        if success:
            # Initialize the orchestrator
            await consciousness_orchestrator.initialize_consciousness()
            
            return {
                "status": "success",
                "message": "Consciousness system initialized successfully",
                "initialized": True
            }
        else:
            return {
                "status": "error", 
                "message": "Failed to initialize consciousness system",
                "initialized": False
            }
            
    except Exception as e:
        logging.error(f"Manual consciousness initialization error: {e}")
        return {
            "status": "error",
            "message": str(e),
            "initialized": False
        }

@router.get("/consciousness/debug")
async def debug_consciousness_state():
    """Debug endpoint to check consciousness state data"""
    try:
        from backend.utils.consciousness_orchestrator_fixed import consciousness_orchestrator_fixed as consciousness_orchestrator
        from backend.utils.neo4j_production import neo4j_production

        # Get consciousness state from orchestrator
        orchestrator_state = await consciousness_orchestrator.get_consciousness_state()
        
        # Get raw data from Neo4j
        raw_query = """
        MATCH (ms:MainzaState {state_id: 'mainza-state-1'})
        RETURN ms.consciousness_level AS consciousness_level,
               ms.emotional_state AS emotional_state,
               ms.total_interactions AS total_interactions,
               ms.learning_rate AS learning_rate,
               ms.evolution_level AS evolution_level
        """
        
        raw_result = neo4j_production.execute_query(raw_query)
        
        return {
            "orchestrator_state": {
                "consciousness_level": orchestrator_state.consciousness_level if orchestrator_state else None,
                "emotional_state": orchestrator_state.emotional_state if orchestrator_state else None,
                "total_interactions": orchestrator_state.total_interactions if orchestrator_state else None,
            } if orchestrator_state else None,
            "raw_neo4j_data": raw_result[0] if raw_result else None,
            "neo4j_connection": "working" if raw_result is not None else "failed"
        }
        
    except Exception as e:
        logging.error(f"Debug consciousness error: {e}")
        return {"error": str(e), "traceback": traceback.format_exc()}

@router.get("/consciousness/knowledge-graph-stats")
async def get_knowledge_graph_stats():
    """Get real knowledge graph statistics using reliable queries"""
    try:
        from backend.utils.neo4j_production import neo4j_production
        
        # Use separate, reliable queries like the insights endpoint
        node_count_query = "MATCH (n) RETURN count(n) as total_nodes"
        rel_count_query = "MATCH ()-[r]->() RETURN count(r) as total_relationships"
        concept_count_query = "MATCH (c:Concept) RETURN count(c) as concept_count"
        memory_count_query = "MATCH (m:Memory) RETURN count(m) as memory_count"
        
        # Execute queries
        node_result = neo4j_production.execute_query(node_count_query)
        rel_result = neo4j_production.execute_query(rel_count_query)
        concept_result = neo4j_production.execute_query(concept_count_query)
        memory_result = neo4j_production.execute_query(memory_count_query)
        
        # Extract counts
        total_nodes = node_result[0]["total_nodes"] if node_result else 0
        total_relationships = rel_result[0]["total_relationships"] if rel_result else 0
        concept_count = concept_result[0]["concept_count"] if concept_result else 0
        memory_count = memory_result[0]["memory_count"] if memory_result else 0
        
        # Calculate health based on data availability
        health = 60  # Base health
        if total_nodes > 0:
            health += 20
        if total_relationships > 0:
            health += 20
        
        return {
            "concepts": concept_count,
            "memories": memory_count,
            "relationships": total_relationships,
            "health": min(health, 100)
        }
            
    except Exception as e:
        logging.error(f"Failed to get knowledge graph stats: {e}")
        # Fallback stats
        return {
            "concepts": 15,
            "memories": 28,
            "relationships": 22,
            "health": 75
        }

@router.post("/livekit/token", include_in_schema=True)
async def get_livekit_token(request: Request):
    try:
        data = await request.json()
        user_id = data.get("user") or data.get("user_id")
        room = data.get("room", "mainza-ai")
        logging.debug(f"[LiveKitToken] API_KEY: {os.getenv('LIVEKIT_API_KEY')}")
        logging.debug(f"[LiveKitToken] API_SECRET: {os.getenv('LIVEKIT_API_SECRET')[:4]}... (masked)")
        token = generate_access_token(room=room, identity=user_id)
        logging.debug(f"[LiveKitToken] Token: {token}")
        decoded = jwt.decode(token, os.getenv('LIVEKIT_API_SECRET'), algorithms=["HS256"])
        logging.debug(f"[LiveKitToken] Decoded Payload: {decoded}")
        return {"token": token, "url": os.getenv("LIVEKIT_URL")}
    except Exception as e:
        logging.error(f"[LiveKitToken] Error: {e}")
        return JSONResponse(status_code=500, content={"error": str(e)})

@router.post("/livekit/audio")
def livekit_audio(file: UploadFile = File(...), user_id: str = "user", agent: str = "router"):
    """
    Accepts audio, transcribes, processes with agent, synthesizes response, returns audio.
    """
    try:
        # Save uploaded audio to temp file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(file.file.read())
            tmp_path = tmp.name
        # Transcribe
        transcript = whisper_model.transcribe(tmp_path)["text"]
        # Route to agent
        agent_result = router_agent.run(transcript)
        if hasattr(agent_result, 'output'):
            response_text = str(agent_result.output)
        else:
            response_text = str(agent_result)
        # Synthesize (if TTS is available)
        if coqui_tts_model is not None:
            tts_wav = coqui_tts_model.tts(response_text, speaker="random", language="en")
            # Save TTS to temp file
            tts_path = tmp_path + "_tts.wav"
            coqui_tts_model.save_wav(tts_wav, tts_path)
            # Return audio file
            return FileResponse(tts_path, media_type="audio/wav", filename="ai_response.wav")
        else:
            # Return text response if TTS not available
            return JSONResponse(content={"text": response_text, "message": "TTS not available"})
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@router.post("/livekit/ai-audio")
def livekit_ai_audio(text: str):
    """
    Accepts text, synthesizes with TTS, returns audio.
    """
    try:
        if coqui_tts_model is None:
            return JSONResponse(status_code=503, content={"error": "TTS not available in this deployment"})
        
        tts_wav = coqui_tts_model.tts(text, speaker="random", language="en")
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            coqui_tts_model.save_wav(tts_wav, tmp.name)
            tts_path = tmp.name
        # Convert to browser-compatible PCM WAV (16-bit, 44.1kHz, mono)
        with tempfile.NamedTemporaryFile(delete=False, suffix="_pcm.wav") as pcm_tmp:
            pcm_path = pcm_tmp.name
        try:
            subprocess.run([
                "ffmpeg", "-y", "-i", tts_path, "-ar", "44100", "-ac", "1", "-sample_fmt", "s16", pcm_path
            ], check=True, capture_output=True)
            os.remove(tts_path)
            tts_path = pcm_path
        except Exception as e:
            if os.path.exists(pcm_path):
                os.remove(pcm_path)
            return JSONResponse(status_code=500, content={"error": f"ffmpeg conversion error: {str(e)}"})
        return FileResponse(tts_path, media_type="audio/wav", filename="ai_response.wav")
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@router.post("/tts/generate")
def generate_tts_audio(text: str = Body(..., embed=True)):
    """
    Accepts text, synthesizes with TTS, and returns the audio file directly.
    """
    try:
        if coqui_tts_model is None:
            return JSONResponse(status_code=503, content={"error": "TTS not available in this deployment"})
        
        # Use a temporary file to store the generated WAV data
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            # Synthesize audio using the pre-loaded Coqui TTS model
            coqui_tts_model.tts_to_file(text=text, file_path=tmp.name)
            tts_path = tmp.name
        # Convert to browser-compatible PCM WAV (16-bit, 44.1kHz, mono)
        with tempfile.NamedTemporaryFile(delete=False, suffix="_pcm.wav") as pcm_tmp:
            pcm_path = pcm_tmp.name
        try:
            subprocess.run([
                "ffmpeg", "-y", "-i", tts_path, "-ar", "44100", "-ac", "1", "-sample_fmt", "s16", pcm_path
            ], check=True, capture_output=True)
            os.remove(tts_path)
            tts_path = pcm_path
        except Exception as e:
            if os.path.exists(pcm_path):
                os.remove(pcm_path)
            return JSONResponse(status_code=500, content={"error": f"ffmpeg conversion error: {str(e)}"})
        # Return the audio file as a response that can be played in the browser
        return FileResponse(tts_path, media_type="audio/wav", filename="mainza_response.wav")
    except Exception as e:
        # Log the error for debugging
        print(f"TTS Generation Error: {e}")
        return JSONResponse(status_code=500, content={"error": str(e), "traceback": traceback.format_exc()})

# Export router for inclusion in main.py
